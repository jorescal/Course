IBM Course 6 Data Analysis with Python





cd ~/Desktop/'IBM Course'/Github2/GitReps/Course







# First of all, create new Repo on Github

cd ~/Desktop/Github2/GitReps/	# create a new directory on your computer
mkdri Course			# every new directory needs to be create from the command line
cd Course 



git init
git config --global user.email "jorescal@gmail.com"


ls				# make sure you have the files you want to upload on the directory you are seeting


echo "# Data Science Course" >> README.md
git add README.md
git commit  -m "first commit of course notes xx/xx/2020"



git add 'IBM Course 6 Data Analysis with Python.txt'
git add 'for_loops.ipynb'

git commit  -m "commit # upedate XX of course notes date xx/xx/2020"

for_loops.ipynb

git remote add origin git@github.com:jorescal/Course.git
git push -u origin master
















********** for every day update 


cd ~/Desktop/Github2/GitReps/Course


git add 'IBM Course 6 Data Analysis with Python.txt'
git add 'for_loops.ipynb'
git commit  -m "commit # XX of course notes date xx/xx/2020"
git push 
















*********************** Week 1  - Python Packages for Data Science



We have divided the Python data analysis libraries into three groups.



	Scientific computing libraries




* Pandas : The primary instrument of Pandas is the two dimensional table consisting of column and row labels, which are called a data frame. 



* NumPy: uses arrays for its inputs and outputs. 



* SciPy: includes functions for some advanced math problems as well as data visualization.






	data visualization methods



* Matplotlib package: is the most well known library for data visualization. 

* Seaborn: It is based on Matplotlib. It's very easy to generate various plots such as heat maps, time series and violin plots.







	machine learning algorithms



* Scikit-learn library: contains tools statistical modeling, including regression, classification, clustering, and so on. This library is built on NumPy, SciPy and Matplotib. 



* Statsmodels: is also a Python module that allows users to explore data, estimate statistical models, and perform statistical tests.






*** *** *** *** read any data using python's pandas package. 






	#from the command line run the following commannds 


import pandas as pd
url = "https://onc/it.com/admin/conta.csv"    #specify the url where the data frame is located
df = pd.read_csv*(url)






	# if the data frame does not have a header run the following command

df = pd.read_csv (url, header - None)






	# Print (n) row from the data frame (n = the number of row from top to bottom)

df.head(10)		# 10 = the number of row from top to bottom
df.tail(13)		# 13 = the number of row from top to bottom



	


	# Assdign colum names 


headers = ["brand", "fuel", "shift", "Price" ]
df.columns = headers
df.head(5)





	# Export data fram to new csv file\

path = "C/windows/.../cars.csv"
df.to_csv(path)






	# check for Data types

df.dtypes





	# Summary for all columns 

df.describe(include = "all")

	# And

df.info()







*** *** *** *** Accessing Databases with Python


You use connection objects to connect to a database and manage your transactions. 


Cursor objects are used to run queries. Cursors are used to scan through the results of a database.



Here are the methods used with connection objects. 

* The cursor() method returns a new cursor object using the connection. 

* The commit() method is used to commit any pending transaction to the database. 

* The rollback() method causes the database to roll back to the start of any pending transaction.

* The close() method is used to close a database connection. 












*********************** Week 2  -  "Data Wrangling "




### How to replace missing values 

### Use "dataframe.replace(missing_value, new_value )"


mean = df ["Normaliza-lost"].mean ( )       # in order to obtain the mea() value of the column
df ["Normaliza-lost"].replace(np.nan, mean) # replace column contain "nan" with mean () value 






### Apply calculation to an entire column 


df[ "city" ] = 235/df [ "city" ]
df.rename(columns = ( "city" : "city-km"), inplace = True)




                              # To identify data types use the command 

dataframe.dtypes( )


                              # To convert data types use the command 

dataframe.astype ( )


### Example:

                              # cast each element in the column "price" to an integer

df["price"] = df["price"].astype("int")


                              # add "1"to each element in the column df["a"] and assign it back to the column df["a"]

df["a"]=df["a"]+1









### ### ### ### Methods of normalizing data.



### "simple feature scaling"

                              # Consider the column "length"; the correct code for "simple feature scaling":

df["length"] = df["length"]/df["length"].max() 


###  "Min-Max"


?????




### "Z-score".
                              # Consider the column "length"; the correct code for "Z-score".

df["length"] = (df["length"]-df["length"].mean())/df["length"].std()







### ### ### ### Binning in Python Pandas 


bins = np.linspace(min(df["prise"]), max(df["prise"]), 4)

group_names= ["Low", "Medium", "High"]

df["prise-binned"] = pd.cut(df["prise"], bins, labels = group_names, include_lowest = True)










### ### ### ### Dummy Variables in Pythion pandas 



        

                       # the correct line of code to perform One-hot encoding on the column 'fuel'


# Before Dummy Variables command

#'fuel'
# gas
# disel
# gas 
# gas 


pd.get_dummies(df['fuel'])

# After Dummy Variables command

# gas	disel
# 1	0
# 0	1
# 1	0
# 1	0





*********************** Week 3  -  Exploratory Data Analysis (EDA)
 	 




describe function in pandas. Using the describe function and applying it on your data frame, the describe function:


df.describe()


automatically computes basic statistics for all numerical variables. It shows:
 
* the mean, 
* the total number of data points, 
* the standard deviation, 
* the quartiles and the extreme values. 

Any NAN values are automatically skipped in these statistics. 











### categorical variables in a data set

### Example, Summarize the categorical data by using the "value_count()" method:


drive_wheels_counts = df ["drive-wheels"].values_counts().to_frame()


drive_wheels_counts.rename(columns = {"drive-wheels": "values_counts" }, inplace = True)
drive_wheels_counts


# 		values_counts
# drive_wheels
# 
# fwd		118
# rwd		75
# 4wd		8



### Examplecode segment to generate Box plots :



sns.boxplot(x="car-style", y="price", data=df)



### visualize this is using a scatter plot. Each observation in the scatter plot is represented as a point. This plot shows the relationship between two variables. The predictor variable, is the variable that you are using to predict an outcome.In a scatter plot, we typically set the predictor variable on the x-axis or horizontal axis, and we set the target variable on the y-axis or vertical axis.



x=df[“engine-size”]
y=df[“price”]
plt.scatter(x,y)
plt.title(“Scatterplot of Engine Size vs Price”)
plt.xlabel(“Engine Size”) 
plt.ylabel(“Price”) 






### Basics of grouping 

### The group by method is used on categorical variables, groups the data into subsets according to the different categories of that variable. You can group by a single variable or you can group by multiple variables by passing in multiple variable names. 




### Examplecode segment of groupby function used to find the average "price" of each car based on "car-style"


df[['price','body-style']].groupby(['body-style'],as_index= False).mean()



### Transform the table to a pivot table that has one variable displayed along the columns and the other variable displayed along the rows.


????


### visualize using a heat map plot...Heat map takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points. 


????



### correlations statistical methods. using a method called Pearson correlation (two values: the correlation coefficient and the P-value). 

* a value close to 1 implies a large positive correlation, 
* while a value close to negative 1 implies a large negative correlation, 
* and a value close to zero implies no correlation between the variables. 


P-value will tell us how certain we are about the correlation that we calculated. For the P-value, 

* a value less than.001 gives us a strong certainty about the correlation coefficient that we calculated. 
* A value between.001 and.05 gives us moderate certainty. 
* A value between.05 and.1 will give us a weak certainty. 
* And a P-value larger than.1 will give us no certainty of correlation at all.



### Examplecode segment for Pearson correlation

pearson_coef, p_value = stats.pearsonr(df["horsepower"], df ["prise"])


# sample result will be 
# * Pearson correlatio: 0.81
# * P-value: 9.35e-48
